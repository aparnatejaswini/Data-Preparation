{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Data preparation refers to transforming raw data into a form that is better suited to predictive modeling.\n",
    "\n",
    "On a predictive modeling project, such as classification or regression, raw data typically cannot be used directly.\n",
    "\n",
    "There are four main reasons why this is the case:\n",
    "\n",
    " - <strong>Data Types</strong>: Machine learning algorithms require data to be numbers.\n",
    " - <strong>Data Requirements</strong>: Some machine learning algorithms impose requirements on the data.\n",
    " - <strong>Data Errors</strong>: Statistical noise and errors in the data may need to be corrected.\n",
    " - <strong>Data Complexity</strong>: Complex nonlinear relationships may be teased out of the data.<br><br>\n",
    "The raw data must be pre-processed prior to being used to fit and evaluate a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing includes:\n",
    "\n",
    " - <strong>Data Cleaning</strong>: Identifying and correcting mistakes or errors in the data.\n",
    " - <strong>Feature Selection</strong>: Identifying those input variables that are most relevant to the task.\n",
    " - <strong>Data Transforms</strong>: Changing the scale or distribution of variables.\n",
    " - <strong>Feature Engineering</strong>: Deriving new variables from available data.\n",
    " - <strong>Dimensionality Reduction</strong>: Creating compact projections of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Identify and deal with missing values\n",
    "<ul>\n",
    "<li>Identify if dataset has any missing values.</li>\n",
    "    <li>Decide on whether to keep or delete that particular record/feature from the dataset.</li>\n",
    "</ul>\n",
    "Filling missing values with data is called <strong>data imputation</strong> and a popular approach for data imputation is to calculate a statistical value for each column (such as a mean) and replace all missing values for that column with the statistic.<br>\n",
    "\n",
    "### Types of missing data\n",
    "<strong>Missing Completely At Random (MCAR)</strong>: A variable is missing completely at random (MCAR) if the probability of being missing is the same for all the observations. There’s no relationship between whether a data point is missing and any values in the data set, missing or observed. Missing values are randomly distributed across all observations, then we consider the data to be missing completely at random. A quick check for this is to compare two parts of data – one with missing observations and the other without missing observations. On a t-test, if we do not find any difference in means between the two samples of data, we can assume the data to be MCAR. \n",
    "\n",
    "<strong>Missing At Random (MAR)</strong>: The key difference between MCAR and MAR is that under MAR the data is not missing randomly across all observations, but is missing randomly only within sub-samples of data. For example, if high school GPA data is missing randomly across all schools in a district, that data will be considered MCAR. However, if data is randomly missing for students in specific schools of the district, then the data is MAR. The propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data.\n",
    "\n",
    "<strong>Not Missing At Random (NMAR)</strong>: When the missing data has a structure to it, we cannot treat it as missing at random. In the above example, if the data was missing for all students from specific schools, then the data cannot be treated as MAR.\n",
    "<pre>\n",
    "Pattern  /   Data Explains Pattern\n",
    "\n",
    "            Yes         No\n",
    "\n",
    "Yes         MAR        NMAR\n",
    "\n",
    "No          --         MCAR\n",
    "</pre>\n",
    " if there is a pattern to a variable's missingness and the data we have cannot explain it we have NMAR, but if the data we have (i.e. other variables in our data set) can explain it we have MAR. If there is no pattern to the missingness, it's MCAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:27]\n",
    "y = df.iloc[:,27]\n",
    "display(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "display(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf= RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = SVC()\n",
    "sv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = BaggingClassifier()\n",
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are prevented from evaluating LDA, Random Forest, Decision Tree, KNN, GuassianNB, SVC, Logistic Regression, Adaboost classifier, Bagging classifier, GradientBoost classifier on a dataset that contains missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(f\"Accuracy Score of XGBoost {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Statistical imputation for missing values\n",
    "\n",
    "Common statistics calculated include:\n",
    "<ul>\n",
    "    <li>The column mean value.</li>\n",
    "    <li>The column median value.</li>\n",
    "    <li>The column mode value.</li>\n",
    "    <li>A constant value.</li>\n",
    "</ul>\n",
    "Mean and median are suitable for numerical variable.<br>\n",
    "If the data follows normal distribution, mean and median are approximately same. If the data is skewed then median is better choice.<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strategies = ['mean', 'median', 'most_frequent', 'constant']\n",
    "titanic_df = pd.read_csv(\"titanic-train.csv\", usecols=['Age', 'Fare', 'Survived', 'Embarked'])\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df['Age'].plot(kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Numerical features\n",
    "### 1. statistical imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute = SimpleImputer(strategy='median')\n",
    "temp_df = pd.DataFrame(impute.fit_transform(titanic_df[['Survived', 'Age','Fare']]), columns = ['Survived', 'Age', 'Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df['Age'].plot(kind='kde', color='c', label='Age Before Imputation')\n",
    "temp_df['Age'].plot(kind='kde', color='m', label='Age After Median Imputation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Arbitrary value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute = SimpleImputer(strategy='constant', fill_value=99)\n",
    "temp_df = pd.DataFrame(impute.fit_transform(titanic_df[['Survived', 'Age', 'Fare']]), columns = ['Survived', 'Age', 'Fare'])\n",
    "titanic_df['Age'].plot(kind='kde', color='c', label='Age Before Imputation')\n",
    "temp_df['Age'].plot(kind='kde', color='m', label='Age After Imputation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  End of tail imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme = titanic_df.Age.mean()+3*titanic_df.Age.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan_endtail(df, col, extreme):\n",
    "    df[col+\"_extreme\"]= df[col].fillna(extreme)\n",
    "    df[col+\"_median\"]= df[col].fillna(df[col].median())\n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_nan_endtail(titanic_df, 'Age', extreme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df['Age'].plot(kind='kde', color ='g', label='Before Imputation')\n",
    "titanic_df['Age_median'].plot(kind='kde', color ='r', label='Median Imputation')\n",
    "titanic_df['Age_extreme'].plot(kind='kde', color ='b', label='End tail Imputation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For categorical features\n",
    "### 4. Most frequent Category imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute=SimpleImputer(strategy='most_frequent')\n",
    "temp_df = pd.DataFrame(impute.fit_transform(titanic_df[['Embarked']]), columns=['Embarked'])\n",
    "print(temp_df['Embarked'].value_counts())\n",
    "print(titanic_df['Embarked'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Missing category imputation\n",
    "Treats missing values as a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute=SimpleImputer(strategy='constant', fill_value='Missing')\n",
    "temp_df = pd.DataFrame(impute.fit_transform(titanic_df[['Embarked']]), columns=['Embarked'])\n",
    "print(temp_df['Embarked'].value_counts())\n",
    "print(titanic_df['Embarked'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For both numerical and categorical features\n",
    "### 6. Complete Case Analysis\n",
    "\n",
    "Only include observations with no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = titanic_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp_df.shape, titanic_df.shape,temp_df.isna().sum(), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Random Sample Imputation\n",
    "\n",
    "Take random samples from observations and use the randomly selected values to fill in the missing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = titanic_df['Age'].dropna().sample(titanic_df['Age'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan_randomsample(df, col):\n",
    "    random_sample = df[col].dropna().sample(df[col].isnull().sum())\n",
    "    df[col+\"_random\"] = df[col]\n",
    "    random_sample.index = df[df[col].isnull()].index\n",
    "    df.loc[df[col].isnull(), col+\"_random\"] = random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_nan_randomsample(titanic_df, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df['Age'].plot(kind='kde', color ='g', label='Before Imputation')\n",
    "titanic_df['Age_median'].plot(kind='kde', color ='r', label='Median Imputation')\n",
    "titanic_df['Age_extreme'].plot(kind='kde', color ='b', label='End tail Imputation')\n",
    "titanic_df['Age_random'].plot(kind='kde', color ='y', label='Random sample Imputation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Missing Indicator\n",
    "A missing indicator is an additional binary variable that indicates whether the data was\n",
    "missing for an observation (1) or not (0). The goal here is to capture observations\n",
    "where data is missing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "indicator = MissingIndicator()\n",
    "temp_df = pd.DataFrame(indicator.fit_transform(titanic_df))\n",
    "ind_cols = [col+\"_NA_IND\" for col in titanic_df.columns[indicator.features_] ]\n",
    "temp_df.columns = ind_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. KNN Imputer\n",
    "\n",
    "    - Works with numerical features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "knnimp = KNNImputer()\n",
    "temp_df = pd.DataFrame(knnimp.fit_transform(titanic_df[['Age']]), columns=['Age'])\n",
    "\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnimp.fit_transform(titanic_df[['Embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#titanic_df.Embarked =titanic_df['Embarked'].map({'S':0, 'C':1, 'Q':2})\n",
    "#titanic_df['Embarked'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(knnimp.fit_transform(titanic_df[['Embarked']]), columns=['Embarked'])\n",
    "temp_df.Embarked.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Iterative Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "itimpute = IterativeImputer()\n",
    "print(titanic_df.isna().sum())\n",
    "temp_df=pd.DataFrame(itimpute.fit_transform(titanic_df), columns=titanic_df.columns.tolist())\n",
    "print(temp_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
